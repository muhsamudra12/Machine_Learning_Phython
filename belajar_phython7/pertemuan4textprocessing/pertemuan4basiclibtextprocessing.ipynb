{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c95f1df2-56b9-4c41-baa0-57f7db3b7b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "berikut ini adalah 5 rekomendasi anime bagus dengan  adalah dainanaoji, chiyou mahou, nisekoi, kimi wa meido-sama, dan hitoribocchi no isekai kouryaku.\n"
     ]
    }
   ],
   "source": [
    "kalimat = \"Berikut ini adalah 5 Rekomendasi Anime bagus dengan  adalah Dainanaoji, Chiyou Mahou, Nisekoi, Kimi wa Meido-sama, dan Hitoribocchi no Isekai Kouryaku.\"\n",
    "lower_case = kalimat.lower()\n",
    "print(lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21bdc215-62ac-4b4d-92b1-9fa42fcdb9c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Berikut ini adalah  Rekomendasi Anime bagus dengan  adalah Dainanaoji, Chiyou Mahou, Nisekoi, Kimi wa Meido-sama, dan Hitoribocchi no Isekai Kouryaku.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re \n",
    "kalimat = \"Berikut ini adalah 5 Rekomendasi Anime bagus dengan  adalah Dainanaoji, Chiyou Mahou, Nisekoi, Kimi wa Meido-sama, dan Hitoribocchi no Isekai Kouryaku.\"\n",
    "\n",
    "hasil = re.sub(r\"\\d+\", \"\", kalimat)\n",
    "hasil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "acef1f75-91c5-4f34-94f0-e3557bc21074",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ini adalah contoh Anime dengan manga dan doijinshi'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "kalimat = \"Ini &adalah [contoh] Anime? {dengan} manga. dan doijinshi?!!\"\n",
    "hasil = kalimat.translate(str.maketrans(\"\",\"\",string.punctuation))\n",
    "hasil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a2240aa-0ede-4b3e-becf-ffaaa65e2b88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ini Anime saya yang bagus'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kalimat = \" \\t    ini Anime saya yang bagus  \\t   \\n\"\n",
    "hasil = kalimat.strip()\n",
    "\n",
    "hasil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "da2d8f98-9b23-495b-8116-74b69f25183d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Yandere',\n",
       " 'adalah',\n",
       " 'sifat',\n",
       " 'yang',\n",
       " 'saya',\n",
       " 'suka',\n",
       " 'sejak',\n",
       " 'menonton',\n",
       " 'Anime']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kalimat = \"Yandere adalah sifat yang saya suka sejak menonton Anime\"\n",
    "pisah = kalimat.split()\n",
    "\n",
    "pisah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ff5b61d-3382-4c1b-ba3c-a87400f06cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\user\\anaconda3\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\user\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\user\\anaconda3\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\anaconda3\\lib\\site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ce1d1038-c302-44f7-951c-08629c0f7a36",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\User/nltk_data'\n    - 'C:\\\\Users\\\\User\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\User\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\User\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\User\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[62], line 6\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m word_tokenize \n\u001b[0;32m      4\u001b[0m kalimat \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAndi kerap melakukan transaksi rutin secara daring atau online.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 6\u001b[0m tokens \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mtokenize\u001b[38;5;241m.\u001b[39mword_tokenize(kalimat)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:142\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 142\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    144\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[0;32m    145\u001b[0m     ]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m _get_punkt_tokenizer(language)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[1;34m(language)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_punkt_tokenizer\u001b[39m(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;124;03m    a lru cache for performance.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;124;03m    :type language: str\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m PunktTokenizer(language)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m-> 1744\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_lang(lang)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1747\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[1;32m-> 1749\u001b[0m     lang_dir \u001b[38;5;241m=\u001b[39m find(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizers/punkt_tab/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlang\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1750\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params \u001b[38;5;241m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[0;32m   1751\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang \u001b[38;5;241m=\u001b[39m lang\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\User/nltk_data'\n    - 'C:\\\\Users\\\\User\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\User\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\User\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\User\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "kalimat = \"Andi kerap melakukan transaksi rutin secara daring atau online.\"\n",
    "\n",
    "tokens = nltk.tokenize.word_tokenize(kalimat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6e32a749-2950-4eb4-b781-ec2771eaa613",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "889ab568-e5a5-479e-8f4c-93ffee89a882",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokens' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[60], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m tokens\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tokens' is not defined"
     ]
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "73fba7f3-be1c-4e25-abfe-62d7a3659a34",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\User/nltk_data'\n    - 'C:\\\\Users\\\\User\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\User\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\User\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\User\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m kalimat \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAndi kerap melakukan transaksi rutin secara daring atau online.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      4\u001b[0m kalimat \u001b[38;5;241m=\u001b[39m kalimat\u001b[38;5;241m.\u001b[39mtranslate(\u001b[38;5;28mstr\u001b[39m\u001b[38;5;241m.\u001b[39mmaketrans(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m,string\u001b[38;5;241m.\u001b[39mpunctuation))\u001b[38;5;241m.\u001b[39mlower()\n\u001b[1;32m----> 6\u001b[0m tokens \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mtokenize\u001b[38;5;241m.\u001b[39mword_tokenize(kalimat)\n\u001b[0;32m      8\u001b[0m tokens\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:142\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 142\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    144\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[0;32m    145\u001b[0m     ]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m _get_punkt_tokenizer(language)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[1;34m(language)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_punkt_tokenizer\u001b[39m(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;124;03m    a lru cache for performance.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;124;03m    :type language: str\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m PunktTokenizer(language)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m-> 1744\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_lang(lang)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1747\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[1;32m-> 1749\u001b[0m     lang_dir \u001b[38;5;241m=\u001b[39m find(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizers/punkt_tab/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlang\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1750\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params \u001b[38;5;241m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[0;32m   1751\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang \u001b[38;5;241m=\u001b[39m lang\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\User/nltk_data'\n    - 'C:\\\\Users\\\\User\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\User\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\User\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\User\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize \n",
    " \n",
    "kalimat = \"Andi kerap melakukan transaksi rutin secara daring atau online.\"\n",
    "kalimat = kalimat.translate(str.maketrans('','',string.punctuation)).lower()\n",
    "\n",
    "tokens = nltk.tokenize.word_tokenize(kalimat)\n",
    "\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "703974c1-58ad-46c7-8715-bfa907d19393",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\User/nltk_data'\n    - 'C:\\\\Users\\\\User\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\User\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\User\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\User\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m kalimat \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAndi kerap melakukan transaksi rutin secara daring atau online. Menurut Andi belanja online lebih praktis & murah.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      5\u001b[0m kalimat \u001b[38;5;241m=\u001b[39m kalimat\u001b[38;5;241m.\u001b[39mtranslate(\u001b[38;5;28mstr\u001b[39m\u001b[38;5;241m.\u001b[39mmaketrans(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m,string\u001b[38;5;241m.\u001b[39mpunctuation))\u001b[38;5;241m.\u001b[39mlower()\n\u001b[1;32m----> 7\u001b[0m tokens \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mtokenize\u001b[38;5;241m.\u001b[39mword_tokenize(kalimat)\n\u001b[0;32m      8\u001b[0m kemunculan \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mFreqDist(tokens)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(kemunculan\u001b[38;5;241m.\u001b[39mmost_common())\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:142\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 142\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    144\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[0;32m    145\u001b[0m     ]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m _get_punkt_tokenizer(language)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[1;34m(language)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_punkt_tokenizer\u001b[39m(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;124;03m    a lru cache for performance.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;124;03m    :type language: str\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m PunktTokenizer(language)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m-> 1744\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_lang(lang)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1747\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[1;32m-> 1749\u001b[0m     lang_dir \u001b[38;5;241m=\u001b[39m find(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizers/punkt_tab/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlang\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1750\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params \u001b[38;5;241m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[0;32m   1751\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang \u001b[38;5;241m=\u001b[39m lang\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\User/nltk_data'\n    - 'C:\\\\Users\\\\User\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\User\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\User\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\User\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "kalimat = \"Andi kerap melakukan transaksi rutin secara daring atau online. Menurut Andi belanja online lebih praktis & murah.\"\n",
    "kalimat = kalimat.translate(str.maketrans('','',string.punctuation)).lower()\n",
    "\n",
    "tokens = nltk.tokenize.word_tokenize(kalimat)\n",
    "kemunculan = nltk.FreqDist(tokens)\n",
    "\n",
    "print(kemunculan.most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a2f8fdb3-4023-4b41-b871-a518f8b5f280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Roaming\\nltk_data\\tokenizers\\punkt\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "print(nltk.data.find('tokenizers/punkt'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cabad06c-9145-4655-bb6c-36f356482046",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (13821229.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[52], line 2\u001b[1;36m\u001b[0m\n\u001b[1;33m    nltk.download('punkt', download_dir=''C:\\\\Users\\\\User/nltk_data', 'C:\\\\Users\\\\User\\\\anaconda3\\\\nltk_data', 'C:\\\\Users\\\\User\\\\anaconda3\\\\share\\\\nltk_data', 'C:\\\\Users\\\\User\\\\anaconda3\\\\lib\\\\nltk_data', 'C:\\\\Users\\\\User\\\\AppData\\\\Roaming\\\\nltk_data', 'C:\\\\nltk_data', 'D:\\\\nltk_data', 'E:\\\\nltk_data'')  # Sesuaikan dengan path nltk kamu\u001b[0m\n\u001b[1;37m                                        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt', download_dir=''C:\\\\Users\\\\User/nltk_data', 'C:\\\\Users\\\\User\\\\anaconda3\\\\nltk_data', 'C:\\\\Users\\\\User\\\\anaconda3\\\\share\\\\nltk_data', 'C:\\\\Users\\\\User\\\\anaconda3\\\\lib\\\\nltk_data', 'C:\\\\Users\\\\User\\\\AppData\\\\Roaming\\\\nltk_data', 'C:\\\\nltk_data', 'D:\\\\nltk_data', 'E:\\\\nltk_data'')  # Sesuaikan dengan path nltk kamu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2e8ea2a5-2607-4da6-96f6-04645565a9da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C:\\\\Users\\\\User/nltk_data', 'C:\\\\Users\\\\User\\\\anaconda3\\\\nltk_data', 'C:\\\\Users\\\\User\\\\anaconda3\\\\share\\\\nltk_data', 'C:\\\\Users\\\\User\\\\anaconda3\\\\lib\\\\nltk_data', 'C:\\\\Users\\\\User\\\\AppData\\\\Roaming\\\\nltk_data', 'C:\\\\nltk_data', 'D:\\\\nltk_data', 'E:\\\\nltk_data']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "print(nltk.data.path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9bd7d871-58ad-4c8d-8047-a10c378f023a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Buat data acak untuk contoh\n",
    "kemunculan = np.random.randn(500)\n",
    "\n",
    "# Plot histogram\n",
    "plt.hist(kemunculan, bins=50, cumulative=False)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "18319f49-e168-4f55-ba3e-12b16b95d540",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement matplotlib.pyplot (from versions: none)\n",
      "ERROR: No matching distribution found for matplotlib.pyplot\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "11e66e14-749a-4968-a146-6b9774d142c0",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1538993276.py, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[72], line 7\u001b[1;36m\u001b[0m\n\u001b[1;33m    print(tokens)from nltk.tokenize import sent_tokenize\u001b[0m\n\u001b[1;37m                 ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "kalimat = \"Andi kerap melakukan transaksi rutin secara daring atau online. Menurut Andi belanja online lebih praktis & murah.\"\n",
    "\n",
    "tokens = nltk.tokenize.sent_tokenize(kalimat)\n",
    "\n",
    "print(tokens)from nltk.tokenize import sent_tokenize\n",
    "\n",
    "kalimat = \"Andi kerap melakukan transaksi rutin secara daring atau online. Menurut Andi belanja online lebih praktis & murah.\"\n",
    "\n",
    "tokens = nltk.tokenize.sent_tokenize(kalimat)\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "efa6eb8c-b695-491c-81f3-bd42ee788aae",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\User/nltk_data'\n    - 'C:\\\\Users\\\\User\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\User\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\User\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\User\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[74], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m kalimat \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAndi dan icha kerap melakukan transaksi rutin secara daring atau online. Menurut Andi belanja online lebih praktis & murah.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      5\u001b[0m kalimat \u001b[38;5;241m=\u001b[39m kalimat\u001b[38;5;241m.\u001b[39mtranslate(\u001b[38;5;28mstr\u001b[39m\u001b[38;5;241m.\u001b[39mmaketrans(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m,string\u001b[38;5;241m.\u001b[39mpunctuation))\u001b[38;5;241m.\u001b[39mlower()\n\u001b[1;32m----> 7\u001b[0m tokens \u001b[38;5;241m=\u001b[39m word_tokenize(kalimat)\n\u001b[0;32m      8\u001b[0m listStopword \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(stopwords\u001b[38;5;241m.\u001b[39mwords(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIndonesian\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     10\u001b[0m removed \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:142\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 142\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    144\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[0;32m    145\u001b[0m     ]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m _get_punkt_tokenizer(language)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[1;34m(language)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_punkt_tokenizer\u001b[39m(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;124;03m    a lru cache for performance.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;124;03m    :type language: str\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m PunktTokenizer(language)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m-> 1744\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_lang(lang)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1747\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[1;32m-> 1749\u001b[0m     lang_dir \u001b[38;5;241m=\u001b[39m find(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizers/punkt_tab/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlang\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1750\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params \u001b[38;5;241m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[0;32m   1751\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang \u001b[38;5;241m=\u001b[39m lang\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\User/nltk_data'\n    - 'C:\\\\Users\\\\User\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\User\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\User\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\User\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    " \n",
    "kalimat = \"Andi dan icha kerap melakukan transaksi rutin secara daring atau online. Menurut Andi belanja online lebih praktis & murah.\"\n",
    "kalimat = kalimat.translate(str.maketrans('','',string.punctuation)).lower()\n",
    "\n",
    "tokens = word_tokenize(kalimat)\n",
    "listStopword = set(stopwords.words('Indonesian'))\n",
    "\n",
    "removed = []\n",
    "for t in tokens:\n",
    "    if t not in listStopword:\n",
    "        removed.append(t)\n",
    " \n",
    "print(removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5050f961-1c3c-4449-bf78-b1f9932ca9fb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'Sastrawi'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[76], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mSastrawi\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mStopWordRemover\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mStopWordRemoverFactory\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StopWordRemoverFactory\n\u001b[0;32m      3\u001b[0m factory \u001b[38;5;241m=\u001b[39m StopWordRemoverFactory()\n\u001b[0;32m      4\u001b[0m stopwords \u001b[38;5;241m=\u001b[39m factory\u001b[38;5;241m.\u001b[39mget_stop_words()\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'Sastrawi'"
     ]
    }
   ],
   "source": [
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "\n",
    "factory = StopWordRemoverFactory()\n",
    "stopwords = factory.get_stop_words()\n",
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5220611a-1ba9-439e-9a31-e837dc2f5f9c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'Sastrawi'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[78], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mSastrawi\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mStopWordRemover\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mStopWordRemoverFactory\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StopWordRemoverFactory\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m word_tokenize \n\u001b[0;32m      4\u001b[0m factory \u001b[38;5;241m=\u001b[39m StopWordRemoverFactory()\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'Sastrawi'"
     ]
    }
   ],
   "source": [
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "factory = StopWordRemoverFactory()\n",
    "stopword = factory.create_stop_word_remover()\n",
    "\n",
    "kalimat = \"Andi kerap melakukan transaksi rutin secara daring atau online. Menurut Andi belanja online lebih praktis & murah.\"\n",
    "kalimat = kalimat.translate(str.maketrans('','',string.punctuation)).lower()\n",
    "\n",
    "stop = stopword.remove(kalimat)\n",
    "tokens = nltk.tokenize.word_tokenize(stop)\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "673c616e-dcda-4216-b65e-544646e1b525",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'Sastrawi'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[80], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mSastrawi\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mStopWordRemover\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mStopWordRemoverFactory\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StopWordRemoverFactory, StopWordRemover, ArrayDictionary\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m word_tokenize \n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# ambil stopword bawaan\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'Sastrawi'"
     ]
    }
   ],
   "source": [
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory, StopWordRemover, ArrayDictionary\n",
    "from nltk.tokenize import word_tokenize \n",
    "    \n",
    "# ambil stopword bawaan\n",
    "stop_factory = StopWordRemoverFactory().get_stop_words()\n",
    "more_stopword = ['daring', 'online']\n",
    "\n",
    "kalimat = \"Andi kerap melakukan transaksi rutin secara daring atau online. Menurut Andi belanja online lebih praktis & murah.\"\n",
    "kalimat = kalimat.translate(str.maketrans('','',string.punctuation)).lower()\n",
    "\n",
    "# menggabungkan stopword\n",
    "data = stop_factory + more_stopword\n",
    "\n",
    "dictionary = ArrayDictionary(data)\n",
    "str = StopWordRemover(dictionary)\n",
    "tokens = nltk.tokenize.word_tokenize(str.remove(kalimat))\n",
    " \n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "016dd840-6b05-4f6b-840e-be8929ba046e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "program  :  program\n",
      "programs  :  program\n",
      "programer  :  program\n",
      "programing  :  program\n",
      "programers  :  program\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer    \n",
    "ps = PorterStemmer() \n",
    "\n",
    "kata = [\"program\", \"programs\", \"programer\", \"programing\", \"programers\"]\n",
    "\n",
    "for k in kata:\n",
    "    print(k, \" : \", ps.stem(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "979b2428-d828-442b-a145-9e8360751455",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'Sastrawi'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[84], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mSastrawi\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mStemmer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mStemmerFactory\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StemmerFactory\n\u001b[0;32m      2\u001b[0m factory \u001b[38;5;241m=\u001b[39m StemmerFactory()\n\u001b[0;32m      3\u001b[0m stemmer \u001b[38;5;241m=\u001b[39m factory\u001b[38;5;241m.\u001b[39mcreate_stemmer()\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'Sastrawi'"
     ]
    }
   ],
   "source": [
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    " \n",
    "kalimat = \"Andi kerap melakukan transaksi rutin secara daring atau online. Menurut Andi belanja online lebih praktis & menyenangkan.\"\n",
    "\n",
    "hasil = stemmer.stem(kalimat)\n",
    "\n",
    "hasil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717f5d24-7e2e-41a3-a2ac-9660c7233be7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
